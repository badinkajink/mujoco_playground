{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1-2 Collision Avoidance Training Notebook\n",
    "\n",
    "This notebook:\n",
    "1. Runs MVP tests to validate the environment\n",
    "2. Trains an asymmetric actor-critic policy with PPO\n",
    "3. Evaluates and visualizes the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "xla_flags += \" --xla_gpu_triton_gemm_any=True\"\n",
    "os.environ[\"XLA_FLAGS\"] = xla_flags\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "import mujoco\n",
    "import wandb\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from etils import epath\n",
    "from flax.training import orbax_utils\n",
    "from IPython.display import clear_output, display\n",
    "from orbax import checkpoint as ocp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mujoco_playground import locomotion, wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "# Enable persistent compilation cache\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: MVP Testing\n",
    "\n",
    "Run basic tests to validate the environment before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration:\n",
      "  ctrl_dt: 0.01\n",
      "  episode_length: 300\n",
      "  history_len: 3\n",
      "  action_scale: 0.6\n",
      "  traj_dir: /home/wxie/workspace/h1_mujoco/augmented\n"
     ]
    }
   ],
   "source": [
    "# Test configuration\n",
    "env_name = \"H12SkinAvoid\"\n",
    "env_cfg = locomotion.get_default_config(env_name)\n",
    "\n",
    "# Override trajectory directory if needed\n",
    "# env_cfg.traj_dir = \"./traj_logs\"\n",
    "\n",
    "print(\"Environment configuration:\")\n",
    "print(f\"  ctrl_dt: {env_cfg.ctrl_dt}\")\n",
    "print(f\"  episode_length: {env_cfg.episode_length}\")\n",
    "print(f\"  history_len: {env_cfg.history_len}\")\n",
    "print(f\"  action_scale: {env_cfg.action_scale}\")\n",
    "print(f\"  traj_dir: {env_cfg.traj_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1205 13:42:06.887194  163510 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W1205 13:42:06.891368  163423 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling trajectories by 5x: 500.0Hz -> 100.0Hz\n",
      "Cache not found, loading from CSVs...\n",
      "Loading 1600 trajectory files from /home/wxie/workspace/h1_mujoco/augmented...\n",
      "Successfully loaded 1600 trajectories\n",
      "Trajectory lengths: min=22, max=69, mean=34.7\n",
      "Trajectory database ready: {'num_trajectories': 1600, 'min_length': 22, 'max_length': 69, 'mean_length': 34.6875, 'std_length': 12.158941268920898, 'total_timesteps': 55500}\n",
      "Saving trajectory database to cache: /home/wxie/workspace/h1_mujoco/augmented/traj_cache_ds5.pkl\n",
      "Cache saved\n",
      "Loaded trajectory database: {'num_trajectories': 1600, 'min_length': 22, 'max_length': 69, 'mean_length': 34.6875, 'std_length': 12.158941268920898, 'total_timesteps': 55500}\n",
      "Found 63 skin sensors\n",
      "✓ Environment loaded successfully\n",
      "  Controlled joints: 21\n",
      "  Skin sensors: 63\n",
      "  Action size: 21\n",
      "  Trajectories loaded: 1600\n",
      "\n",
      "Trajectory database stats:\n",
      "  num_trajectories: 1600\n",
      "  min_length: 22\n",
      "  max_length: 69\n",
      "  mean_length: 34.6875\n",
      "  std_length: 12.158941268920898\n",
      "  total_timesteps: 55500\n"
     ]
    }
   ],
   "source": [
    "# Load environment\n",
    "print(\"Loading environment...\")\n",
    "env = locomotion.load(env_name, config=env_cfg)\n",
    "\n",
    "print(f\"✓ Environment loaded successfully\")\n",
    "print(f\"  Controlled joints: {env._num_controlled_joints}\")\n",
    "print(f\"  Skin sensors: {env._num_skin_sensors}\")\n",
    "print(f\"  Action size: {env.action_size}\")\n",
    "print(f\"  Trajectories loaded: {env._traj_db.num_trajectories}\")\n",
    "\n",
    "# Print trajectory database stats\n",
    "stats = env._traj_db.get_stats()\n",
    "print(\"\\nTrajectory database stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing reset...\n",
      "✓ Reset successful\n",
      "  Observation keys: dict_keys(['state', 'privileged_state'])\n",
      "  Actor obs shape: (534,)\n",
      "  Critic obs shape: (540,)\n",
      "  Trajectory length: 21\n",
      "\n",
      "Computed observation sizes:\n",
      "  Actor: 534 (actual: 534)\n",
      "  Critic: 540 (actual: 540)\n",
      "✓ Observation shapes validated\n"
     ]
    }
   ],
   "source": [
    "# Test reset\n",
    "print(\"Testing reset...\")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = env.reset(rng)\n",
    "\n",
    "print(f\"✓ Reset successful\")\n",
    "print(f\"  Observation keys: {state.obs.keys()}\")\n",
    "print(f\"  Actor obs shape: {state.obs['state'].shape}\")\n",
    "print(f\"  Critic obs shape: {state.obs['privileged_state'].shape}\")\n",
    "print(f\"  Trajectory length: {state.info['traj_length']}\")\n",
    "\n",
    "# Check observation sizes\n",
    "actor_size, critic_size = env._compute_obs_size()\n",
    "print(f\"\\nComputed observation sizes:\")\n",
    "print(f\"  Actor: {actor_size} (actual: {state.obs['state'].shape[0]})\")\n",
    "print(f\"  Critic: {critic_size} (actual: {state.obs['privileged_state'].shape[0]})\")\n",
    "\n",
    "assert state.obs['state'].shape[0] == actor_size, \"Actor obs size mismatch!\"\n",
    "assert state.obs['privileged_state'].shape[0] == critic_size, \"Critic obs size mismatch!\"\n",
    "print(\"✓ Observation shapes validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing step...\n",
      "✓ Step successful\n",
      "  Reward: -6.9424\n",
      "  Done: 0.0\n",
      "  Trajectory step: 1/21\n",
      "\n",
      "  Top reward components:\n",
      "    reward/torque: -692.729065\n",
      "    reward/energy: -2.475880\n",
      "    reward/joint_pos_tracking: 0.946265\n",
      "    reward/base_vel_tracking: 0.935290\n",
      "    reward/action_rate: -0.916446\n",
      "    reward/joint_limit: 0.003135\n",
      "    reward/base_angvel_tracking: 0.000000\n",
      "    reward/clearance_reward: 0.000000\n",
      "    reward/collision_penalty: -0.000000\n",
      "    reward/joint_vel_tracking: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Test step\n",
    "print(\"Testing step...\")\n",
    "rng, action_rng = jax.random.split(rng)\n",
    "action = jax.random.uniform(action_rng, shape=(env._num_controlled_joints,), minval=-1.0, maxval=1.0)\n",
    "next_state = env.step(state, action)\n",
    "\n",
    "print(f\"✓ Step successful\")\n",
    "print(f\"  Reward: {next_state.reward:.4f}\")\n",
    "print(f\"  Done: {next_state.done}\")\n",
    "print(f\"  Trajectory step: {next_state.info['traj_step']}/{next_state.info['traj_length']}\")\n",
    "\n",
    "# Print top reward components\n",
    "print(\"\\n  Top reward components:\")\n",
    "reward_items = [(k, float(v)) for k, v in next_state.metrics.items() if k.startswith('reward/')]\n",
    "reward_items.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "for k, v in reward_items[:10]:\n",
    "    print(f\"    {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing capacitance sensing...\n",
      "  Num sensors: 63\n",
      "  Num detections: 0\n",
      "  Max capacitance: -1.0000\n",
      "  No detections (obstacle far from robot)\n",
      "✓ Capacitance computation working\n"
     ]
    }
   ],
   "source": [
    "# Test capacitance computation\n",
    "print(\"Testing capacitance sensing...\")\n",
    "capacitances = state.info['capacitances']\n",
    "\n",
    "print(f\"  Num sensors: {len(capacitances)}\")\n",
    "print(f\"  Num detections: {jp.sum(capacitances > 0.0):.0f}\")\n",
    "print(f\"  Max capacitance: {jp.max(capacitances):.4f}\")\n",
    "min_nonzero = jp.min(jp.where(capacitances > 0, capacitances, jp.inf))\n",
    "if jp.isfinite(min_nonzero):\n",
    "    print(f\"  Min (non-zero) capacitance: {min_nonzero:.4f}\")\n",
    "else:\n",
    "    print(f\"  No detections (obstacle far from robot)\")\n",
    "\n",
    "print(f\"✓ Capacitance computation working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing JIT compilation...\n",
      "✓ JIT compilation successful\n"
     ]
    }
   ],
   "source": [
    "# Test JIT compilation\n",
    "print(\"Testing JIT compilation...\")\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = jit_reset(rng)\n",
    "action = jp.zeros(env._num_controlled_joints)\n",
    "state = jit_step(state, action)\n",
    "\n",
    "print(\"✓ JIT compilation successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 50-step rollout...\n",
      "  Episode terminated at step 13\n",
      "✓ Rollout completed\n",
      "  Steps: 13\n",
      "  Total reward: -60.32\n",
      "  Avg reward: -4.6396\n",
      "  Collisions: 0\n"
     ]
    }
   ],
   "source": [
    "# Test short rollout\n",
    "print(\"Testing 50-step rollout...\")\n",
    "rng = jax.random.PRNGKey(123)\n",
    "state = jit_reset(rng)\n",
    "\n",
    "total_reward = 0.0\n",
    "collision_count = 0\n",
    "\n",
    "for step in range(50):\n",
    "    rng, action_rng = jax.random.split(rng)\n",
    "    action = jax.random.uniform(action_rng, shape=(env._num_controlled_joints,), minval=-1.0, maxval=1.0)\n",
    "    state = jit_step(state, action)\n",
    "    total_reward += float(state.reward)\n",
    "    \n",
    "    if float(state.metrics['collision_detected']) > 0.5:\n",
    "        collision_count += 1\n",
    "    \n",
    "    if float(state.done) > 0.5:\n",
    "        print(f\"  Episode terminated at step {step+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"✓ Rollout completed\")\n",
    "print(f\"  Steps: {step+1}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Avg reward: {total_reward/(step+1):.4f}\")\n",
    "print(f\"  Collisions: {collision_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Training Setup\n",
    "\n",
    "Configure PPO training parameters and initialize W&B logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PPO parameters\n",
    "# You'll need to create this in locomotion_params.py\n",
    "# For now, let's create basic parameters\n",
    "from ml_collections import config_dict\n",
    "\n",
    "ppo_params = config_dict.ConfigDict()\n",
    "ppo_params.num_timesteps = 10_000_000  # 10M steps\n",
    "ppo_params.num_evals = 20\n",
    "ppo_params.reward_scaling = 1.0\n",
    "ppo_params.episode_length = env_cfg.episode_length\n",
    "ppo_params.normalize_observations = True\n",
    "ppo_params.action_repeat = 1\n",
    "ppo_params.unroll_length = 10\n",
    "ppo_params.num_minibatches = 32\n",
    "ppo_params.num_updates_per_batch = 4\n",
    "ppo_params.discounting = 0.97\n",
    "ppo_params.learning_rate = 3e-4\n",
    "ppo_params.entropy_cost = 1e-3\n",
    "ppo_params.num_envs = 4096\n",
    "ppo_params.batch_size = 4096\n",
    "ppo_params.seed = 0\n",
    "\n",
    "# Network architecture for asymmetric actor-critic\n",
    "ppo_params.network_factory = config_dict.ConfigDict()\n",
    "ppo_params.network_factory.policy_hidden_layer_sizes = (512, 256, 128)\n",
    "ppo_params.network_factory.value_hidden_layer_sizes = (512, 256, 128)\n",
    "ppo_params.network_factory.activation = 'swish'\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"PPO Parameters:\")\n",
    "pprint(ppo_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if you've added to locomotion_params.py:\n",
    "# ppo_params = locomotion_params.brax_ppo_config(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup wandb logging\n",
    "USE_WANDB = True  # Set to True to enable W&B logging\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(project=\"mjxrl-avoid\", config=env_cfg)\n",
    "    wandb.config.update({\"env_name\": env_name})\n",
    "    wandb.config.update(ppo_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpointing\n",
    "SUFFIX = None  # Add suffix to experiment name if desired\n",
    "FINETUNE_PATH = None  # Set to checkpoint path to resume training\n",
    "\n",
    "# Generate unique experiment name\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "exp_name = f\"{env_name}-{timestamp}\"\n",
    "if SUFFIX is not None:\n",
    "    exp_name += f\"-{SUFFIX}\"\n",
    "print(f\"Experiment name: {exp_name}\")\n",
    "\n",
    "# Possibly restore from checkpoint\n",
    "if FINETUNE_PATH is not None:\n",
    "    FINETUNE_PATH = epath.Path(FINETUNE_PATH)\n",
    "    latest_ckpts = list(FINETUNE_PATH.glob(\"*\"))\n",
    "    latest_ckpts = [ckpt for ckpt in latest_ckpts if ckpt.is_dir()]\n",
    "    latest_ckpts.sort(key=lambda x: int(x.name))\n",
    "    latest_ckpt = latest_ckpts[-1]\n",
    "    restore_checkpoint_path = latest_ckpt\n",
    "    print(f\"Restoring from: {restore_checkpoint_path}\")\n",
    "else:\n",
    "    restore_checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "ckpt_path = epath.Path(\"checkpoints\").resolve() / exp_name\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint path: {ckpt_path}\")\n",
    "\n",
    "# Save config\n",
    "with open(ckpt_path / \"config.json\", \"w\") as fp:\n",
    "    json.dump(env_cfg.to_dict(), fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Training\n",
    "\n",
    "Train the asymmetric actor-critic policy with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup progress tracking\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    \"\"\"Progress callback for training.\"\"\"\n",
    "    # Log to wandb\n",
    "    if USE_WANDB:\n",
    "        wandb.log(metrics, step=num_steps)\n",
    "\n",
    "    # Plot\n",
    "    clear_output(wait=True)\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, ppo_params.num_timesteps * 1.25])\n",
    "    # plt.ylim([0, 100])  # Adjust based on your expected reward range\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "\n",
    "    display(plt.gcf())\n",
    "\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "    \"\"\"Checkpoint callback.\"\"\"\n",
    "    del make_policy  # Unused\n",
    "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(params)\n",
    "    path = ckpt_path / f\"{current_step}\"\n",
    "    orbax_checkpointer.save(path, params, force=True, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training function\n",
    "# Get domain randomization if available\n",
    "try:\n",
    "    randomizer = locomotion.get_domain_randomizer(env_name)\n",
    "except:\n",
    "    randomizer = None\n",
    "    print(\"No domain randomizer found, using None\")\n",
    "\n",
    "training_params = dict(ppo_params)\n",
    "del training_params[\"network_factory\"]\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    **training_params,\n",
    "    network_factory=functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    ),\n",
    "    restore_checkpoint_path=restore_checkpoint_path,\n",
    "    progress_fn=progress,\n",
    "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    policy_params_fn=policy_params_fn,\n",
    "    randomization_fn=randomizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total timesteps: {ppo_params.num_timesteps:,}\")\n",
    "print(f\"Number of environments: {ppo_params.num_envs}\")\n",
    "print(f\"Batch size: {ppo_params.batch_size}\")\n",
    "\n",
    "env = locomotion.load(env_name, config=env_cfg)\n",
    "eval_env = locomotion.load(env_name, config=env_cfg)\n",
    "make_inference_fn, params, _ = train_fn(environment=env, eval_env=eval_env)\n",
    "\n",
    "if len(times) > 1:\n",
    "    print(f\"\\nTime to jit: {times[1] - times[0]}\")\n",
    "    print(f\"Time to train: {times[-1] - times[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot of reward vs wallclock time\n",
    "plt.figure()\n",
    "# plt.ylim([0, 100])\n",
    "plt.xlabel(\"wallclock time (s)\")\n",
    "plt.ylabel(\"reward per episode\")\n",
    "plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "plt.errorbar(\n",
    "    [(t - times[0]).total_seconds() for t in times[:-1]],\n",
    "    y_data,\n",
    "    yerr=y_dataerr,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalizer and policy params\n",
    "import pickle\n",
    "\n",
    "normalizer_params, policy_params, value_params = params\n",
    "with open(ckpt_path / \"params.pkl\", \"wb\") as f:\n",
    "    data = {\n",
    "        \"normalizer_params\": normalizer_params,\n",
    "        \"policy_params\": policy_params,\n",
    "        \"value_params\": value_params,\n",
    "    }\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"Parameters saved to {ckpt_path / 'params.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation\n",
    "\n",
    "Evaluate the trained policy and visualize rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inference\n",
    "inference_fn = make_inference_fn(params, deterministic=True)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "eval_env = locomotion.load(env_name, config=env_cfg)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation rollouts\n",
    "print(\"Running evaluation rollouts...\")\n",
    "rng = jax.random.PRNGKey(12345)\n",
    "rollout = []\n",
    "rewards = []\n",
    "tracking_rewards = []\n",
    "recovery_rewards = []\n",
    "collision_rewards = []\n",
    "capacitances_max = []\n",
    "tracking_errors = []\n",
    "\n",
    "num_episodes = 10\n",
    "for ep in tqdm(range(num_episodes)):\n",
    "    rng, reset_rng = jax.random.split(rng)\n",
    "    state = jit_reset(reset_rng)\n",
    "    \n",
    "    for i in range(env_cfg.episode_length):\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        state = jit_step(state, ctrl)\n",
    "        \n",
    "        rollout.append(state)\n",
    "        rewards.append(\n",
    "            {k[7:]: float(v) for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        tracking_rewards.append(float(state.metrics['reward/joint_pos_tracking']))\n",
    "        recovery_rewards.append(float(state.metrics['reward/recovery_upright']))\n",
    "        collision_rewards.append(float(state.metrics['reward/collision_penalty']))\n",
    "        capacitances_max.append(float(jp.max(state.info['capacitances'])))\n",
    "        tracking_errors.append(float(state.metrics['tracking_error']))\n",
    "        \n",
    "        if state.done:\n",
    "            print(f\"  Episode {ep} terminated at step {i}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nCollected {len(rollout)} timesteps from {num_episodes} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward components over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Tracking reward\n",
    "axes[0, 0].plot(tracking_rewards)\n",
    "axes[0, 0].set_title('Joint Position Tracking Reward')\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Recovery reward\n",
    "axes[0, 1].plot(recovery_rewards)\n",
    "axes[0, 1].set_title('Recovery Upright Reward')\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Reward')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Collision penalty\n",
    "axes[1, 0].plot(collision_rewards)\n",
    "axes[1, 0].set_title('Collision Penalty')\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].set_ylabel('Penalty')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', label='No collision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Max capacitance\n",
    "axes[1, 1].plot(capacitances_max)\n",
    "axes[1, 1].set_title('Max Capacitance (Proximity)')\n",
    "axes[1, 1].set_xlabel('Timestep')\n",
    "axes[1, 1].set_ylabel('Capacitance')\n",
    "axes[1, 1].axhline(y=env._cap_collision_threshold, color='r', linestyle='--', label='Collision threshold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tracking error over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(tracking_errors)\n",
    "plt.title('Joint Position Tracking Error')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('L2 Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean tracking error: {jp.mean(jp.array(tracking_errors)):.4f}\")\n",
    "print(f\"Std tracking error: {jp.std(jp.array(tracking_errors)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze collision statistics\n",
    "num_collisions = sum(1 for r in collision_rewards if r < 0)\n",
    "collision_rate = num_collisions / len(collision_rewards)\n",
    "\n",
    "print(f\"Collision Statistics:\")\n",
    "print(f\"  Total timesteps: {len(collision_rewards)}\")\n",
    "print(f\"  Collisions detected: {num_collisions}\")\n",
    "print(f\"  Collision rate: {collision_rate*100:.2f}%\")\n",
    "print(f\"  Max capacitance observed: {max(capacitances_max):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render video\n",
    "print(\"Rendering video...\")\n",
    "render_every = 2\n",
    "fps = 1.0 / eval_env.dt / render_every\n",
    "traj = rollout[::render_every]\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True\n",
    "scene_option.geomgroup[3] = False\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = False\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "\n",
    "frames = eval_env.render(\n",
    "    traj, camera=\"side\", scene_option=scene_option, height=480, width=640\n",
    ")\n",
    "media.show_video(frames, fps=fps, loop=False)\n",
    "\n",
    "# Optionally save video\n",
    "# media.write_video(f\"{env_name}.mp4\", frames, fps=fps, qp=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Analysis\n",
    "\n",
    "Additional analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reward distribution\n",
    "if rewards:\n",
    "    # Get all unique reward keys\n",
    "    all_keys = set()\n",
    "    for r in rewards:\n",
    "        all_keys.update(r.keys())\n",
    "    \n",
    "    # Compute mean and std for each component\n",
    "    print(\"Reward Component Statistics:\")\n",
    "    print(f\"{'Component':<30} {'Mean':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "    print(\"-\" * 78)\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        values = [r.get(key, 0.0) for r in rewards]\n",
    "        values = jp.array(values)\n",
    "        print(f\"{key:<30} {jp.mean(values):>12.4f} {jp.std(values):>12.4f} {jp.min(values):>12.4f} {jp.max(values):>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of capacitance readings\n",
    "plt.figure(figsize=(10, 4))\n",
    "cap_nonzero = [c for c in capacitances_max if c > 0]\n",
    "if cap_nonzero:\n",
    "    plt.hist(cap_nonzero, bins=50)\n",
    "    plt.axvline(x=env._cap_collision_threshold, color='r', linestyle='--', label='Collision threshold')\n",
    "    plt.xlabel('Capacitance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Max Capacitance Readings (non-zero only)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No non-zero capacitance readings detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'num_episodes': num_episodes,\n",
    "    'total_timesteps': len(rollout),\n",
    "    'collision_rate': float(collision_rate),\n",
    "    'mean_tracking_error': float(jp.mean(jp.array(tracking_errors))),\n",
    "    'std_tracking_error': float(jp.std(jp.array(tracking_errors))),\n",
    "    'max_capacitance': float(max(capacitances_max)) if capacitances_max else 0.0,\n",
    "}\n",
    "\n",
    "with open(ckpt_path / \"eval_results.json\", \"w\") as f:\n",
    "    json.dump(analysis_results, f, indent=4)\n",
    "\n",
    "print(f\"\\nAnalysis results saved to {ckpt_path / 'eval_results.json'}\")\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "for key, value in analysis_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"W&B run finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_playground (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
