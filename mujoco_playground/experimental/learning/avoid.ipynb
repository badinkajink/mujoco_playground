{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1-2 Collision Avoidance Training Notebook\n",
    "\n",
    "This notebook:\n",
    "1. Runs MVP tests to validate the environment\n",
    "2. Trains an asymmetric actor-critic policy with PPO\n",
    "3. Evaluates and visualizes the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "xla_flags += \" --xla_gpu_triton_gemm_any=True\"\n",
    "os.environ[\"XLA_FLAGS\"] = xla_flags\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mujoco_playground import locomotion, wrapper\n",
    "from mujoco_playground.config import locomotion_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment...\n",
      "Loaded 103 assets for H12 Skin.\n",
      "Assets: ['h1_2_skin_mjx copy.xml', 'scene_mjx.xml', 'h1_2_skin_mjx.xml', 'h1_2_skin.xml', 'scene.xml', 'left_shoulder_pitch_link.STL', 'link18_R.STL', 'link19_R.STL', 'link12_L.STL', 'L_thumb_proximal.STL', 'left_ankle_B_link.STL', 'L_hand_base_link.STL', 'left_hip_pitch_link.STL', 'right_wrist_pitch_link.STL', 'right_shoulder_roll_link.STL', 'R_ring_intermediate.STL', 'link20_R.STL', 'left_ankle_A_rod_link.STL', 'link15_R.STL', 'link22_L.STL', 'left_knee_link.STL', 'R_ring_proximal.STL', 'link22_R.STL', 'L_thumb_intermediate.STL', 'left_wrist_roll_link.STL', 'right_ankle_A_rod_link.STL', 'right_ankle_pitch_link.STL', 'link21_L.STL', 'left_ankle_roll_link.STL', 'left_ankle_B_rod_link.STL', 'L_pinky_intermediate.STL', 'link16_R.STL', 'R_index_proximal.STL', 'R_pinky_proximal.STL', 'left_hip_roll_link.STL', 'link12_R.STL', 'link17_L.STL', 'wrist_yaw_link.STL', 'left_shoulder_roll_link.STL', 'R_pinky_intermediate.STL', 'right_pitch_link.STL', 'R_middle_intermediate.STL', 'right_hip_roll_link.STL', 'right_ankle_roll_link.STL', 'link11_L.STL', 'left_elbow_link.STL', 'right_hip_yaw_link.STL', 'shoulder_yaw_R_skin.stl', 'L_index_intermediate.STL', 'R_thumb_proximal.STL', 'right_wrist_roll_link.STL', 'torso_skin.stl', 'L_ring_proximal.STL', 'link13_R.STL', 'left_wrist_pitch_link.STL', 'R_thumb_proximal_base.STL', 'right_elbow_link.STL', 'shoulder_yaw_L_skin.stl', 'right_hip_pitch_link.STL', 'left_ankle_A_link.STL', 'torso_link.STL', 'right_shoulder_pitch_link.STL', 'right_ankle_B_link.STL', 'L_pinky_proximal.STL', 'left_hand_link.STL', 'logo_link.STL', 'right_shoulder_yaw_link.STL', 'link16_L.STL', 'L_thumb_distal.STL', 'elbow_R_skin.stl', 'L_middle_intermediate.STL', 'left_shoulder_yaw_link.STL', 'shoulder_roll_L_skin.stl', 'R_thumb_distal.STL', 'head_skin.stl', 'R_hand_base_link.STL', 'link11_R.STL', 'link17_R.STL', 'link14_R.STL', 'link21_R.STL', 'R_middle_proximal.STL', 'L_ring_intermediate.STL', 'right_hand_link.STL', 'shoulder_roll_R_skin.stl', 'right_knee_link.STL', 'link19_L.STL', 'link18_L.STL', 'left_ankle_pitch_link.STL', 'elbow_L_skin.stl', 'L_thumb_proximal_base.STL', 'link14_L.STL', 'link20_L.STL', 'link13_L.STL', 'right_ankle_link.STL', 'R_index_intermediate.STL', 'right_ankle_B_rod_link.STL', 'left_hip_yaw_link.STL', 'link15_L.STL', 'R_thumb_intermediate.STL', 'pelvis.STL', 'right_ankle_A_link.STL', 'L_index_proximal.STL', 'L_middle_proximal.STL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1204 18:51:24.678091   80589 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W1204 18:51:24.681279   80464 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling trajectories by 5x: 500.0Hz -> 100.0Hz\n",
      "Loading 1600 trajectory files from /home/wxie/workspace/h1_mujoco/augmented...\n",
      "Successfully loaded 1600 trajectories\n",
      "Trajectory lengths: min=22, max=69, mean=34.7\n",
      "Loaded trajectory database: {'num_trajectories': 1600, 'min_length': 22, 'max_length': 69, 'mean_length': 34.6875, 'std_length': 12.158940897545312, 'total_timesteps': 55500}\n",
      "Found 63 skin sensors\n"
     ]
    }
   ],
   "source": [
    "# Load environment\n",
    "print(\"Loading environment...\")\n",
    "env_name = \"H12SkinAvoid\"\n",
    "env_cfg = locomotion.get_default_config(env_name)\n",
    "env = locomotion.load(env_name, config=env_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "import mujoco\n",
    "import wandb\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from etils import epath\n",
    "from flax.training import orbax_utils\n",
    "from IPython.display import clear_output, display\n",
    "from orbax import checkpoint as ocp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mujoco_playground import locomotion, wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "# Enable persistent compilation cache\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: MVP Testing\n",
    "\n",
    "Run basic tests to validate the environment before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration:\n",
      "  ctrl_dt: 0.02\n",
      "  episode_length: 1000\n",
      "  history_len: 3\n",
      "  action_scale: 0.6\n",
      "  traj_dir: ./traj_logs\n"
     ]
    }
   ],
   "source": [
    "# Test configuration\n",
    "env_name = \"H12SkinAvoid\"\n",
    "env_cfg = locomotion.get_default_config(env_name)\n",
    "\n",
    "# Override trajectory directory if needed\n",
    "# env_cfg.traj_dir = \"./traj_logs\"\n",
    "\n",
    "print(\"Environment configuration:\")\n",
    "print(f\"  ctrl_dt: {env_cfg.ctrl_dt}\")\n",
    "print(f\"  episode_length: {env_cfg.episode_length}\")\n",
    "print(f\"  history_len: {env_cfg.history_len}\")\n",
    "print(f\"  action_scale: {env_cfg.action_scale}\")\n",
    "print(f\"  traj_dir: {env_cfg.traj_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment...\n",
      "mujoco_menagerie not found. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning mujoco_menagerie: ██████████| 100/100 [00:31<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out commit e95616395529f0c4093d97a8759b0eb1160a95e6\n",
      "Successfully downloaded mujoco_menagerie\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Error opening file '../mujoco_menagerie/unitree_h1_2_skin/assets/skin/torso_skin.stl': No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load environment\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading environment...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m env = \u001b[43mlocomotion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Environment loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Controlled joints: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv._num_controlled_joints\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/mujoco_playground/mujoco_playground/_src/locomotion/__init__.py:191\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(env_name, config, config_overrides)\u001b[39m\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    188\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnv \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found. Available envs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_cfgs.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m   )\n\u001b[32m    190\u001b[39m config = config \u001b[38;5;129;01mor\u001b[39;00m get_default_config(env_name)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_envs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_overrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/mujoco_playground/mujoco_playground/_src/locomotion/h12_skin/avoid.py:600\u001b[39m, in \u001b[36mAvoid.__init__\u001b[39m\u001b[34m(self, config, config_overrides)\u001b[39m\n\u001b[32m    597\u001b[39m   config = default_config()\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Update XML path to use h12_skin scene\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# TODO: Make sure this XML includes the obstacle and all 63 skin sensors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFEET_ONLY_XML\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Or your custom h12_skin XML\u001b[39;49;00m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28mself\u001b[39m._config = config\n\u001b[32m    606\u001b[39m \u001b[38;5;28mself\u001b[39m._post_init()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/mujoco_playground/mujoco_playground/_src/locomotion/h12_skin/base.py:50\u001b[39m, in \u001b[36mH12SkinEnv.__init__\u001b[39m\u001b[34m(self, xml_path, config, config_overrides)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config, config_overrides)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m._model_assets = get_assets()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28mself\u001b[39m._mj_model = \u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMjModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_xml_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massets\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_assets\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m._mj_model.opt.timestep = \u001b[38;5;28mself\u001b[39m._config.sim_dt\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Increase offscreen framebuffer size to render at higher resolutions.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# TODO(kevin): Consider moving this somewhere else.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Error: Error opening file '../mujoco_menagerie/unitree_h1_2_skin/assets/skin/torso_skin.stl': No such file or directory"
     ]
    }
   ],
   "source": [
    "# Load environment\n",
    "print(\"Loading environment...\")\n",
    "env = locomotion.load(env_name, config=env_cfg)\n",
    "\n",
    "print(f\"✓ Environment loaded successfully\")\n",
    "print(f\"  Controlled joints: {env._num_controlled_joints}\")\n",
    "print(f\"  Skin sensors: {env._num_skin_sensors}\")\n",
    "print(f\"  Action size: {env.action_size}\")\n",
    "print(f\"  Trajectories loaded: {env._traj_db.num_trajectories}\")\n",
    "\n",
    "# Print trajectory database stats\n",
    "stats = env._traj_db.get_stats()\n",
    "print(\"\\nTrajectory database stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reset\n",
    "print(\"Testing reset...\")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = env.reset(rng)\n",
    "\n",
    "print(f\"✓ Reset successful\")\n",
    "print(f\"  Observation keys: {state.obs.keys()}\")\n",
    "print(f\"  Actor obs shape: {state.obs['state'].shape}\")\n",
    "print(f\"  Critic obs shape: {state.obs['privileged_state'].shape}\")\n",
    "print(f\"  Trajectory length: {state.info['traj_length']}\")\n",
    "\n",
    "# Check observation sizes\n",
    "actor_size, critic_size = env._compute_obs_size()\n",
    "print(f\"\\nComputed observation sizes:\")\n",
    "print(f\"  Actor: {actor_size} (actual: {state.obs['state'].shape[0]})\")\n",
    "print(f\"  Critic: {critic_size} (actual: {state.obs['privileged_state'].shape[0]})\")\n",
    "\n",
    "assert state.obs['state'].shape[0] == actor_size, \"Actor obs size mismatch!\"\n",
    "assert state.obs['privileged_state'].shape[0] == critic_size, \"Critic obs size mismatch!\"\n",
    "print(\"✓ Observation shapes validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test step\n",
    "print(\"Testing step...\")\n",
    "rng, action_rng = jax.random.split(rng)\n",
    "action = jax.random.uniform(action_rng, shape=(env._num_controlled_joints,), minval=-1.0, maxval=1.0)\n",
    "next_state = env.step(state, action)\n",
    "\n",
    "print(f\"✓ Step successful\")\n",
    "print(f\"  Reward: {next_state.reward:.4f}\")\n",
    "print(f\"  Done: {next_state.done}\")\n",
    "print(f\"  Trajectory step: {next_state.info['traj_step']}/{next_state.info['traj_length']}\")\n",
    "\n",
    "# Print top reward components\n",
    "print(\"\\n  Top reward components:\")\n",
    "reward_items = [(k, float(v)) for k, v in next_state.metrics.items() if k.startswith('reward/')]\n",
    "reward_items.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "for k, v in reward_items[:10]:\n",
    "    print(f\"    {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test capacitance computation\n",
    "print(\"Testing capacitance sensing...\")\n",
    "capacitances = state.info['capacitances']\n",
    "\n",
    "print(f\"  Num sensors: {len(capacitances)}\")\n",
    "print(f\"  Num detections: {jp.sum(capacitances > 0.0):.0f}\")\n",
    "print(f\"  Max capacitance: {jp.max(capacitances):.4f}\")\n",
    "min_nonzero = jp.min(jp.where(capacitances > 0, capacitances, jp.inf))\n",
    "if jp.isfinite(min_nonzero):\n",
    "    print(f\"  Min (non-zero) capacitance: {min_nonzero:.4f}\")\n",
    "else:\n",
    "    print(f\"  No detections (obstacle far from robot)\")\n",
    "\n",
    "print(f\"✓ Capacitance computation working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JIT compilation\n",
    "print(\"Testing JIT compilation...\")\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = jit_reset(rng)\n",
    "action = jp.zeros(env._num_controlled_joints)\n",
    "state = jit_step(state, action)\n",
    "\n",
    "print(\"✓ JIT compilation successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test short rollout\n",
    "print(\"Testing 50-step rollout...\")\n",
    "rng = jax.random.PRNGKey(123)\n",
    "state = jit_reset(rng)\n",
    "\n",
    "total_reward = 0.0\n",
    "collision_count = 0\n",
    "\n",
    "for step in range(50):\n",
    "    rng, action_rng = jax.random.split(rng)\n",
    "    action = jax.random.uniform(action_rng, shape=(env._num_controlled_joints,), minval=-1.0, maxval=1.0)\n",
    "    state = jit_step(state, action)\n",
    "    total_reward += float(state.reward)\n",
    "    \n",
    "    if float(state.metrics['collision_detected']) > 0.5:\n",
    "        collision_count += 1\n",
    "    \n",
    "    if float(state.done) > 0.5:\n",
    "        print(f\"  Episode terminated at step {step+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"✓ Rollout completed\")\n",
    "print(f\"  Steps: {step+1}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Avg reward: {total_reward/(step+1):.4f}\")\n",
    "print(f\"  Collisions: {collision_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reward scale signs\n",
    "print(\"Checking reward scale signs...\")\n",
    "scales = env._config.reward_config.scales\n",
    "\n",
    "penalty_keys = ['collision_penalty', 'proximity_penalty', 'action_rate', 'torque', 'energy', 'joint_limit']\n",
    "issues = []\n",
    "\n",
    "for key in penalty_keys:\n",
    "    if key in scales:\n",
    "        if scales[key] < 0:\n",
    "            issues.append(f\"  ✗ {key} = {scales[key]} (should be positive)\")\n",
    "        else:\n",
    "            print(f\"  ✓ {key} = {scales[key]}\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\n⚠ WARNING: Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"\\n✓ All reward scales correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Training Setup\n",
    "\n",
    "Configure PPO training parameters and initialize W&B logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PPO parameters\n",
    "# You'll need to create this in locomotion_params.py\n",
    "# For now, let's create basic parameters\n",
    "from ml_collections import config_dict\n",
    "\n",
    "ppo_params = config_dict.ConfigDict()\n",
    "ppo_params.num_timesteps = 100_000_000  # 100M steps\n",
    "ppo_params.num_evals = 20\n",
    "ppo_params.reward_scaling = 1.0\n",
    "ppo_params.episode_length = env_cfg.episode_length\n",
    "ppo_params.normalize_observations = True\n",
    "ppo_params.action_repeat = 1\n",
    "ppo_params.unroll_length = 10\n",
    "ppo_params.num_minibatches = 32\n",
    "ppo_params.num_updates_per_batch = 4\n",
    "ppo_params.discounting = 0.97\n",
    "ppo_params.learning_rate = 3e-4\n",
    "ppo_params.entropy_cost = 1e-3\n",
    "ppo_params.num_envs = 4096\n",
    "ppo_params.batch_size = 4096\n",
    "ppo_params.seed = 0\n",
    "\n",
    "# Network architecture for asymmetric actor-critic\n",
    "ppo_params.network_factory = config_dict.ConfigDict()\n",
    "ppo_params.network_factory.policy_hidden_layer_sizes = (512, 256, 128)\n",
    "ppo_params.network_factory.value_hidden_layer_sizes = (512, 256, 128)\n",
    "ppo_params.network_factory.activation = 'swish'\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"PPO Parameters:\")\n",
    "pprint(ppo_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if you've added to locomotion_params.py:\n",
    "# ppo_params = locomotion_params.brax_ppo_config(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup wandb logging\n",
    "USE_WANDB = True  # Set to True to enable W&B logging\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(project=\"mjxrl-avoid\", config=env_cfg)\n",
    "    wandb.config.update({\"env_name\": env_name})\n",
    "    wandb.config.update(ppo_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpointing\n",
    "SUFFIX = None  # Add suffix to experiment name if desired\n",
    "FINETUNE_PATH = None  # Set to checkpoint path to resume training\n",
    "\n",
    "# Generate unique experiment name\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "exp_name = f\"{env_name}-{timestamp}\"\n",
    "if SUFFIX is not None:\n",
    "    exp_name += f\"-{SUFFIX}\"\n",
    "print(f\"Experiment name: {exp_name}\")\n",
    "\n",
    "# Possibly restore from checkpoint\n",
    "if FINETUNE_PATH is not None:\n",
    "    FINETUNE_PATH = epath.Path(FINETUNE_PATH)\n",
    "    latest_ckpts = list(FINETUNE_PATH.glob(\"*\"))\n",
    "    latest_ckpts = [ckpt for ckpt in latest_ckpts if ckpt.is_dir()]\n",
    "    latest_ckpts.sort(key=lambda x: int(x.name))\n",
    "    latest_ckpt = latest_ckpts[-1]\n",
    "    restore_checkpoint_path = latest_ckpt\n",
    "    print(f\"Restoring from: {restore_checkpoint_path}\")\n",
    "else:\n",
    "    restore_checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "ckpt_path = epath.Path(\"checkpoints\").resolve() / exp_name\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint path: {ckpt_path}\")\n",
    "\n",
    "# Save config\n",
    "with open(ckpt_path / \"config.json\", \"w\") as fp:\n",
    "    json.dump(env_cfg.to_dict(), fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Training\n",
    "\n",
    "Train the asymmetric actor-critic policy with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup progress tracking\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    \"\"\"Progress callback for training.\"\"\"\n",
    "    # Log to wandb\n",
    "    if USE_WANDB:\n",
    "        wandb.log(metrics, step=num_steps)\n",
    "\n",
    "    # Plot\n",
    "    clear_output(wait=True)\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, ppo_params.num_timesteps * 1.25])\n",
    "    # plt.ylim([0, 100])  # Adjust based on your expected reward range\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "\n",
    "    display(plt.gcf())\n",
    "\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "    \"\"\"Checkpoint callback.\"\"\"\n",
    "    del make_policy  # Unused\n",
    "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(params)\n",
    "    path = ckpt_path / f\"{current_step}\"\n",
    "    orbax_checkpointer.save(path, params, force=True, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training function\n",
    "# Get domain randomization if available\n",
    "try:\n",
    "    randomizer = locomotion.get_domain_randomizer(env_name)\n",
    "except:\n",
    "    randomizer = None\n",
    "    print(\"No domain randomizer found, using None\")\n",
    "\n",
    "training_params = dict(ppo_params)\n",
    "del training_params[\"network_factory\"]\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    **training_params,\n",
    "    network_factory=functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    ),\n",
    "    restore_checkpoint_path=restore_checkpoint_path,\n",
    "    progress_fn=progress,\n",
    "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    policy_params_fn=policy_params_fn,\n",
    "    randomization_fn=randomizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total timesteps: {ppo_params.num_timesteps:,}\")\n",
    "print(f\"Number of environments: {ppo_params.num_envs}\")\n",
    "print(f\"Batch size: {ppo_params.batch_size}\")\n",
    "\n",
    "env = locomotion.load(env_name, config=env_cfg)\n",
    "eval_env = locomotion.load(env_name, config=env_cfg)\n",
    "make_inference_fn, params, _ = train_fn(environment=env, eval_env=eval_env)\n",
    "\n",
    "if len(times) > 1:\n",
    "    print(f\"\\nTime to jit: {times[1] - times[0]}\")\n",
    "    print(f\"Time to train: {times[-1] - times[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot of reward vs wallclock time\n",
    "plt.figure()\n",
    "# plt.ylim([0, 100])\n",
    "plt.xlabel(\"wallclock time (s)\")\n",
    "plt.ylabel(\"reward per episode\")\n",
    "plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "plt.errorbar(\n",
    "    [(t - times[0]).total_seconds() for t in times[:-1]],\n",
    "    y_data,\n",
    "    yerr=y_dataerr,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalizer and policy params\n",
    "import pickle\n",
    "\n",
    "normalizer_params, policy_params, value_params = params\n",
    "with open(ckpt_path / \"params.pkl\", \"wb\") as f:\n",
    "    data = {\n",
    "        \"normalizer_params\": normalizer_params,\n",
    "        \"policy_params\": policy_params,\n",
    "        \"value_params\": value_params,\n",
    "    }\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"Parameters saved to {ckpt_path / 'params.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation\n",
    "\n",
    "Evaluate the trained policy and visualize rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inference\n",
    "inference_fn = make_inference_fn(params, deterministic=True)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "eval_env = locomotion.load(env_name, config=env_cfg)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation rollouts\n",
    "print(\"Running evaluation rollouts...\")\n",
    "rng = jax.random.PRNGKey(12345)\n",
    "rollout = []\n",
    "rewards = []\n",
    "tracking_rewards = []\n",
    "recovery_rewards = []\n",
    "collision_rewards = []\n",
    "capacitances_max = []\n",
    "tracking_errors = []\n",
    "\n",
    "num_episodes = 10\n",
    "for ep in tqdm(range(num_episodes)):\n",
    "    rng, reset_rng = jax.random.split(rng)\n",
    "    state = jit_reset(reset_rng)\n",
    "    \n",
    "    for i in range(env_cfg.episode_length):\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        state = jit_step(state, ctrl)\n",
    "        \n",
    "        rollout.append(state)\n",
    "        rewards.append(\n",
    "            {k[7:]: float(v) for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        tracking_rewards.append(float(state.metrics['reward/joint_pos_tracking']))\n",
    "        recovery_rewards.append(float(state.metrics['reward/recovery_upright']))\n",
    "        collision_rewards.append(float(state.metrics['reward/collision_penalty']))\n",
    "        capacitances_max.append(float(jp.max(state.info['capacitances'])))\n",
    "        tracking_errors.append(float(state.metrics['tracking_error']))\n",
    "        \n",
    "        if state.done:\n",
    "            print(f\"  Episode {ep} terminated at step {i}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nCollected {len(rollout)} timesteps from {num_episodes} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward components over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Tracking reward\n",
    "axes[0, 0].plot(tracking_rewards)\n",
    "axes[0, 0].set_title('Joint Position Tracking Reward')\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Recovery reward\n",
    "axes[0, 1].plot(recovery_rewards)\n",
    "axes[0, 1].set_title('Recovery Upright Reward')\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Reward')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Collision penalty\n",
    "axes[1, 0].plot(collision_rewards)\n",
    "axes[1, 0].set_title('Collision Penalty')\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].set_ylabel('Penalty')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', label='No collision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Max capacitance\n",
    "axes[1, 1].plot(capacitances_max)\n",
    "axes[1, 1].set_title('Max Capacitance (Proximity)')\n",
    "axes[1, 1].set_xlabel('Timestep')\n",
    "axes[1, 1].set_ylabel('Capacitance')\n",
    "axes[1, 1].axhline(y=env._cap_collision_threshold, color='r', linestyle='--', label='Collision threshold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tracking error over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(tracking_errors)\n",
    "plt.title('Joint Position Tracking Error')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('L2 Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean tracking error: {jp.mean(jp.array(tracking_errors)):.4f}\")\n",
    "print(f\"Std tracking error: {jp.std(jp.array(tracking_errors)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze collision statistics\n",
    "num_collisions = sum(1 for r in collision_rewards if r < 0)\n",
    "collision_rate = num_collisions / len(collision_rewards)\n",
    "\n",
    "print(f\"Collision Statistics:\")\n",
    "print(f\"  Total timesteps: {len(collision_rewards)}\")\n",
    "print(f\"  Collisions detected: {num_collisions}\")\n",
    "print(f\"  Collision rate: {collision_rate*100:.2f}%\")\n",
    "print(f\"  Max capacitance observed: {max(capacitances_max):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render video\n",
    "print(\"Rendering video...\")\n",
    "render_every = 2\n",
    "fps = 1.0 / eval_env.dt / render_every\n",
    "traj = rollout[::render_every]\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True\n",
    "scene_option.geomgroup[3] = False\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = False\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "\n",
    "frames = eval_env.render(\n",
    "    traj, camera=\"side\", scene_option=scene_option, height=480, width=640\n",
    ")\n",
    "media.show_video(frames, fps=fps, loop=False)\n",
    "\n",
    "# Optionally save video\n",
    "# media.write_video(f\"{env_name}.mp4\", frames, fps=fps, qp=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Analysis\n",
    "\n",
    "Additional analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reward distribution\n",
    "if rewards:\n",
    "    # Get all unique reward keys\n",
    "    all_keys = set()\n",
    "    for r in rewards:\n",
    "        all_keys.update(r.keys())\n",
    "    \n",
    "    # Compute mean and std for each component\n",
    "    print(\"Reward Component Statistics:\")\n",
    "    print(f\"{'Component':<30} {'Mean':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "    print(\"-\" * 78)\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        values = [r.get(key, 0.0) for r in rewards]\n",
    "        values = jp.array(values)\n",
    "        print(f\"{key:<30} {jp.mean(values):>12.4f} {jp.std(values):>12.4f} {jp.min(values):>12.4f} {jp.max(values):>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of capacitance readings\n",
    "plt.figure(figsize=(10, 4))\n",
    "cap_nonzero = [c for c in capacitances_max if c > 0]\n",
    "if cap_nonzero:\n",
    "    plt.hist(cap_nonzero, bins=50)\n",
    "    plt.axvline(x=env._cap_collision_threshold, color='r', linestyle='--', label='Collision threshold')\n",
    "    plt.xlabel('Capacitance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Max Capacitance Readings (non-zero only)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No non-zero capacitance readings detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'num_episodes': num_episodes,\n",
    "    'total_timesteps': len(rollout),\n",
    "    'collision_rate': float(collision_rate),\n",
    "    'mean_tracking_error': float(jp.mean(jp.array(tracking_errors))),\n",
    "    'std_tracking_error': float(jp.std(jp.array(tracking_errors))),\n",
    "    'max_capacitance': float(max(capacitances_max)) if capacitances_max else 0.0,\n",
    "}\n",
    "\n",
    "with open(ckpt_path / \"eval_results.json\", \"w\") as f:\n",
    "    json.dump(analysis_results, f, indent=4)\n",
    "\n",
    "print(f\"\\nAnalysis results saved to {ckpt_path / 'eval_results.json'}\")\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "for key, value in analysis_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"W&B run finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_playground (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
